model_type: "mamba"
model:
  d_model: 512 # 1024
  n_layer: 16
  checkpoint_mixer: True
  residual_in_fp32: False
  vocab_size: 38
  max_position_embeddings: 2048
  max_seq_position_embeddings: 512
  add_position_ids: "1d" #["none, "1d", "2d"]
  reverse: False # when true trains the model in a bidirectional way