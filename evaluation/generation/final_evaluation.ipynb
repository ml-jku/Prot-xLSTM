{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_df = pd.DataFrame(columns = [\"family\", \"family_id\", \"sequence_length_mean\", \"hamming_mean\", \"hmmer_mean\", \"ptm_mean\", \"plddt_mean\",\n",
    "                                        \"sequence_length_median\", \"hamming_median\", \"hmmer_median\", \"ptm_median\", \"plddt_median\",\n",
    "                                        \"sequence_length_std\", \"hamming_std\", \"hmmer_std\", \"ptm_std\", \"plddt_std\",])\n",
    "\n",
    "for family_idx in [20, 22, 50, 98, 100, 141, 177, 222, 233, 265, 303, 327, 338, 341, 376, 393, 471, 479, 481]:\n",
    "    \n",
    "    with open(f\"evaluations/natural/sequence_df_{family_idx}\", \"rb\") as f:\n",
    "        sequence_df = pickle.load(f)\n",
    "\n",
    "    eval_dict = {\"family\": [family_idx], \"family_id\": [sequence_df[\"family_id\"].iloc[0]]}\n",
    "\n",
    "    eval_dict[\"sequence_length_mean\"] = [np.mean(list(sequence_df[\"sequence_length\"]))]\n",
    "    eval_dict[\"hamming_mean\"] = [np.mean(list(sequence_df[\"min_hamming\"]))]\n",
    "    eval_dict[\"hmmer_mean\"] = [np.mean(list(sequence_df[\"score_gen\"]))]\n",
    "    eval_dict[\"ptm_mean\"] = [np.mean(list(sequence_df[\"ptm\"]))]\n",
    "    eval_dict[\"plddt_mean\"] = [np.mean(list(sequence_df[\"mean_plddt\"]))]\n",
    "\n",
    "    eval_dict[\"sequence_length_median\"] = [np.median(list(sequence_df[\"sequence_length\"]))]\n",
    "    eval_dict[\"hamming_median\"] = [np.median(list(sequence_df[\"min_hamming\"]))]\n",
    "    eval_dict[\"hmmer_median\"] = [np.median(list(sequence_df[\"score_gen\"]))]\n",
    "    eval_dict[\"ptm_median\"] = [np.median(list(sequence_df[\"ptm\"]))]\n",
    "    eval_dict[\"plddt_median\"] = [np.median(list(sequence_df[\"mean_plddt\"]))]\n",
    "\n",
    "    eval_dict[\"sequence_length_std\"] = [np.std(list(sequence_df[\"sequence_length\"]))]\n",
    "    eval_dict[\"hamming_std\"] = [np.std(list(sequence_df[\"min_hamming\"]))]\n",
    "    eval_dict[\"hmmer_std\"] = [np.std(list(sequence_df[\"score_gen\"]))]\n",
    "    eval_dict[\"ptm_std\"] = [np.std(list(sequence_df[\"ptm\"]))]\n",
    "    eval_dict[\"plddt_std\"] = [np.std(list(sequence_df[\"mean_plddt\"]))]\n",
    "\n",
    "    natural_df = pd.concat([natural_df, pd.DataFrame(eval_dict)], ignore_index=True)\n",
    "\n",
    "with open(f\"evaluations/natural/final_evaluation\", \"wb\") as f:\n",
    "    pickle.dump(natural_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"evaluations/natural/final_evaluation\", \"rb\") as f:\n",
    "    natural_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['protxlstm_102M_60B', 'protmamba_107M_195B', 'protxlstm_26M_30B', 'protmamba_28M_30B']:\n",
    "\n",
    "    evaluation_df = pd.DataFrame(columns = [\"family\", \"family_id\", \"sequence_length_mean\", \"hamming_mean\", \"hmmer_mean\", \"ptm_mean\", \"plddt_mean\",\n",
    "                                        \"sequence_length_median\", \"hamming_median\", \"hmmer_median\", \"ptm_median\", \"plddt_median\",\n",
    "                                        \"sequence_length_std\", \"hamming_std\", \"hmmer_std\", \"ptm_std\", \"plddt_std\",\n",
    "                                        \"sequence_length_delta\", \"hamming_delta\", \"hmmer_delta\", \"ptm_delta\", \"plddt_delta\",\n",
    "                                        \"sequence_length_ks\", \"hamming_ks\", \"hmmer_ks\", \"ptm_ks\", \"plddt_ks\",\n",
    "                                        \"sequence_length_ppl_r\", \"hamming_ppl_r\", \"hmmer_ppl_r\", \"ptm_ppl_r\", \"plddt_ppl_r\"\n",
    "                                        ])\n",
    "\n",
    "    for family_idx in [20, 22, 50, 98, 100, 141, 177, 222, 233, 265, 303, 327, 338, 341, 376, 393, 471, 479, 481]:\n",
    "\n",
    "        with open(f\"evaluations/{model_name}/sequence_df_{family_idx}\", \"rb\") as f:\n",
    "            sequence_df = pickle.load(f)\n",
    "\n",
    "        with open(f\"evaluations/natural/sequence_df_{family_idx}\", \"rb\") as f:\n",
    "            natural_df = pickle.load(f)\n",
    "\n",
    "        eval_dict = {\"family\": [family_idx], \"family_id\": [sequence_df[\"family_id\"].iloc[0]]}\n",
    "        ptm_filtered_df = sequence_df.loc[sequence_df[\"ptm\"] != 0]\n",
    "\n",
    "        eval_dict[\"sequence_length_ppl_r\"] = [stats.pearsonr(list(ptm_filtered_df[\"sequence_length\"]), list(ptm_filtered_df[\"perplexity\"])).statistic]\n",
    "        eval_dict[\"hamming_ppl_r\"] = [stats.pearsonr(list(ptm_filtered_df[\"min_hamming\"]), list(ptm_filtered_df[\"perplexity\"])).statistic]\n",
    "        eval_dict[\"hmmer_ppl_r\"] = [stats.pearsonr(list(ptm_filtered_df[\"score_gen\"]), list(ptm_filtered_df[\"perplexity\"])).statistic]\n",
    "        eval_dict[\"ptm_ppl_r\"] = [stats.pearsonr(list(ptm_filtered_df[\"ptm\"]), list(ptm_filtered_df[\"perplexity\"])).statistic]\n",
    "        eval_dict[\"plddt_ppl_r\"] = [stats.pearsonr(list(ptm_filtered_df[\"mean_plddt\"]), list(ptm_filtered_df[\"perplexity\"])).statistic]\n",
    "\n",
    "        best_df = ptm_filtered_df.sort_values(by=\"perplexity\")[:100]\n",
    "        eval_dict[\"sequence_length_mean\"] = [np.mean(list(best_df[\"sequence_length\"]))]\n",
    "        eval_dict[\"hamming_mean\"] = [np.mean(list(best_df[\"min_hamming\"]))]\n",
    "        eval_dict[\"hmmer_mean\"] = [np.mean(list(best_df[\"score_gen\"]))]\n",
    "        eval_dict[\"ptm_mean\"] = [np.mean(list(best_df[\"ptm\"]))]\n",
    "        eval_dict[\"plddt_mean\"] = [np.mean(list(best_df[\"mean_plddt\"]))]\n",
    "\n",
    "        eval_dict[\"sequence_length_median\"] = [np.median(list(best_df[\"sequence_length\"]))]\n",
    "        eval_dict[\"hamming_median\"] = [np.median(list(best_df[\"min_hamming\"]))]\n",
    "        eval_dict[\"hmmer_median\"] = [np.median(list(best_df[\"score_gen\"]))]\n",
    "        eval_dict[\"ptm_median\"] = [np.median(list(best_df[\"ptm\"]))]\n",
    "        eval_dict[\"plddt_median\"] = [np.median(list(best_df[\"mean_plddt\"]))]\n",
    "\n",
    "        eval_dict[\"sequence_length_std\"] = [np.std(list(best_df[\"sequence_length\"]))]\n",
    "        eval_dict[\"hamming_std\"] = [np.std(list(best_df[\"min_hamming\"]))]\n",
    "        eval_dict[\"hmmer_std\"] = [np.std(list(best_df[\"score_gen\"]))]\n",
    "        eval_dict[\"ptm_std\"] = [np.std(list(best_df[\"ptm\"]))]\n",
    "        eval_dict[\"plddt_std\"] = [np.std(list(best_df[\"mean_plddt\"]))]\n",
    "\n",
    "        eval_dict[\"sequence_length_delta_mean\"] = [np.mean(list(best_df[\"sequence_length\"]))-np.mean(list(natural_df[\"sequence_length\"]))]\n",
    "        eval_dict[\"hamming_delta_mean\"] = [np.mean(list(best_df[\"min_hamming\"]))-np.mean(list(natural_df[\"min_hamming\"]))]\n",
    "        eval_dict[\"hmmer_delta_mean\"] = [np.mean(list(best_df[\"score_gen\"]))-np.mean(list(natural_df[\"score_gen\"]))]\n",
    "        eval_dict[\"ptm_delta_mean\"] = [np.mean(list(best_df[\"ptm\"]))-np.mean(list(natural_df[\"ptm\"]))]\n",
    "        eval_dict[\"plddt_delta_mean\"] = [np.mean(list(best_df[\"mean_plddt\"]))-np.mean(list(natural_df[\"mean_plddt\"]))]\n",
    "\n",
    "        eval_dict[\"sequence_length_delta_median\"] = [np.median(list(best_df[\"sequence_length\"]))-np.median(list(natural_df[\"sequence_length\"]))]\n",
    "        eval_dict[\"hamming_delta_median\"] = [np.median(list(best_df[\"min_hamming\"]))-np.median(list(natural_df[\"min_hamming\"]))]\n",
    "        eval_dict[\"hmmer_delta_median\"] = [np.median(list(best_df[\"score_gen\"]))-np.median(list(natural_df[\"score_gen\"]))]\n",
    "        eval_dict[\"ptm_delta_median\"] = [np.median(list(best_df[\"ptm\"]))-np.median(list(natural_df[\"ptm\"]))]\n",
    "        eval_dict[\"plddt_delta_median\"] = [np.median(list(best_df[\"mean_plddt\"]))-np.median(list(natural_df[\"mean_plddt\"]))]\n",
    "\n",
    "        eval_dict[\"sequence_length_ks\"] = [stats.kstest(list(best_df[\"sequence_length\"]), list(natural_df[\"sequence_length\"])).statistic]\n",
    "        eval_dict[\"hamming_ks\"] = [stats.kstest(list(best_df[\"min_hamming\"]), list(natural_df[\"min_hamming\"])).statistic]\n",
    "        eval_dict[\"hmmer_ks\"] = [stats.kstest(list(best_df[\"score_gen\"]), list(natural_df[\"score_gen\"])).statistic]\n",
    "        eval_dict[\"ptm_ks\"] = [stats.kstest(list(best_df[\"ptm\"]), list(natural_df[\"ptm\"])).statistic]\n",
    "        eval_dict[\"plddt_ks\"] = [stats.kstest(list(best_df[\"mean_plddt\"]), list(natural_df[\"mean_plddt\"])).statistic]\n",
    "\n",
    "        eval_dict[\"sequence_length_p\"] = [stats.kstest(list(best_df[\"sequence_length\"]), list(natural_df[\"sequence_length\"])).pvalue]\n",
    "        eval_dict[\"hamming_p\"] = [stats.kstest(list(best_df[\"min_hamming\"]), list(natural_df[\"min_hamming\"])).pvalue]\n",
    "        eval_dict[\"hmmer_p\"] = [stats.kstest(list(best_df[\"score_gen\"]), list(natural_df[\"score_gen\"])).pvalue]\n",
    "        eval_dict[\"ptm_p\"] = [stats.kstest(list(best_df[\"ptm\"]), list(natural_df[\"ptm\"])).pvalue]\n",
    "        eval_dict[\"plddt_p\"] = [stats.kstest(list(best_df[\"mean_plddt\"]), list(natural_df[\"mean_plddt\"])).pvalue]\n",
    "\n",
    "        evaluation_df = pd.concat([evaluation_df, pd.DataFrame(eval_dict)], ignore_index=True)\n",
    "\n",
    "    eval_dict\n",
    "\n",
    "    with open(f\"evaluations/{model_name}/final_evaluation\", \"wb\") as f:\n",
    "        pickle.dump(evaluation_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_summary = pd.DataFrame(columns = ['natural', 'protxlstm_26M_30B', 'protmamba_28M_30B', 'protxlstm_102M_60B', 'protmamba_107M_195B', ], \n",
    "                            index=['sequence_length_mean', 'hamming_mean', 'hmmer_mean', 'ptm_mean', 'plddt_mean', \n",
    "        'sequence_length_median', 'hamming_median', 'hmmer_median', 'ptm_median', 'plddt_median',\n",
    "        'sequence_length_delta_mean', 'hamming_delta_mean', 'hmmer_delta_mean', 'ptm_delta_mean', 'plddt_delta_mean', \n",
    "        'sequence_length_delta_median', 'hamming_delta_median', 'hmmer_delta_median', 'ptm_delta_median', 'plddt_delta_median', \n",
    "        'sequence_length_ks', 'hamming_ks', 'hmmer_ks', 'ptm_ks', 'plddt_ks', \n",
    "        'sequence_length_ppl_r', 'hamming_ppl_r', 'hmmer_ppl_r', 'ptm_ppl_r', 'plddt_ppl_r'])\n",
    "\n",
    "for model_name in ['natural', 'protxlstm_26M_30B', 'protmamba_28M_30B', 'protxlstm_102M_60B', 'protmamba_107M_195B']:\n",
    "    with open(f\"evaluations/{model_name}/final_evaluation\", \"rb\") as f:\n",
    "        eval_df = pickle.load(f)\n",
    "\n",
    "    for metric in ['sequence_length_mean', 'hamming_mean', 'hmmer_mean', 'ptm_mean', 'plddt_mean', \n",
    "        'sequence_length_median', 'hamming_median', 'hmmer_median', 'ptm_median', 'plddt_median',\n",
    "        'sequence_length_delta_mean', 'hamming_delta_mean', 'hmmer_delta_mean', 'ptm_delta_mean', 'plddt_delta_mean', \n",
    "        'sequence_length_delta_median', 'hamming_delta_median', 'hmmer_delta_median', 'ptm_delta_median', 'plddt_delta_median', \n",
    "        'sequence_length_ks', 'hamming_ks', 'hmmer_ks', 'ptm_ks', 'plddt_ks', \n",
    "        'sequence_length_ppl_r', 'hamming_ppl_r', 'hmmer_ppl_r', 'ptm_ppl_r', 'plddt_ppl_r']:\n",
    "\n",
    "        if metric in eval_df.columns:\n",
    "            mean = round(np.mean([abs(x) for x in list(eval_df[metric])]), 2)\n",
    "            confidence_interval = round(1.96*np.std([abs(x) for x in list(eval_df[metric])])/np.sqrt(19), 2)\n",
    "            eval_summary.loc[metric, model_name] = '$' + str(mean) + '^{\\pm ' + str(confidence_interval) + '}$'\n",
    "\n",
    "display(eval_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['natural', 'protxlstm_102M_60B', 'protmamba_107M_195B']  #, 'protxlstm_26M_30B', 'protmamba_28M_30B']\n",
    "metrics = ['sequence_length', 'min_hamming', 'score_gen', 'ptm', 'mean_plddt']\n",
    "datasets = [20, 22, 50, 98, 100, 141, 177, 222, 233, 265] #, 303, 327, 338, 341, 376, 393, 471, 479, 481]\n",
    "\n",
    "model_names = {\"natural\": 'Natural Sequences', \"protxlstm_102M_60B\": \"Prot-xLSTM-102M\", \"protmamba_107M_195B\": \"ProtMamba-107M\", \"protxlstm_26M_30B\": \"Prot-xLSTM-26M\", \"protmamba_28M_30B\": \"ProtMamba-28M\"}\n",
    "metric_names = {'sequence_length': 'Sequence Length', 'min_hamming': \"Min. Hamming\", 'score_gen': \"HMMER score\", 'ptm': \"pTM\", 'mean_plddt': 'pLDDT'}\n",
    "dataset_names = {}\n",
    "\n",
    "cd = {\"natural\": \"grey\", \"protxlstm_102M_60B\": (48/255, 115/255, 173/255, 1.0), \"protmamba_107M_195B\": (223/255, 137/255, 83/255, 1.0), \"protxlstm_26M_30B\": (48/255, 115/255, 173/255, 0.5), \"protmamba_28M_30B\": (223/255, 137/255, 83/255, 0.5)}\n",
    "\n",
    "data = []\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            with open(f\"evaluations/{model}/sequence_df_{dataset}\", \"rb\") as f:\n",
    "                sequence_df = pickle.load(f)\n",
    "                if model != \"natural\":\n",
    "                    sequence_df = sequence_df.loc[sequence_df[\"ptm\"] != 0]\n",
    "                    sequence_df = sequence_df.sort_values(by=\"perplexity\")[:100]\n",
    "                else: dataset_names[dataset] = sequence_df[\"family_id\"].iloc[0]\n",
    "            for metric in metrics:\n",
    "                values = list(sequence_df[metric])\n",
    "                for value in values:\n",
    "                    data.append([model_names[model], metric, dataset, value, cd[model]])\n",
    "        except:\n",
    "            for metric in metrics:\n",
    "                values = [0]\n",
    "                for value in values:\n",
    "                    data.append([model_names[model], metric, dataset, value,  cd[model]])\n",
    "            print(f'No data for {model}, family {dataset}, metric {metric}.')\n",
    "\n",
    "df = pd.DataFrame(data, columns=['model', 'metric', 'dataset', 'value', \"color\"])\n",
    "\n",
    "fig, axes = plt.subplots(len(datasets), len(metrics), figsize=(12,15), sharey='col')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "palette = sns.color_palette(\"Set2\", len(models))\n",
    "\n",
    "for j, dataset in enumerate(datasets):  # Datasets on the y-axis\n",
    "    \n",
    "    for i, metric in enumerate(metrics):  # Metrics on the x-axis\n",
    "        ax = axes[j, i]\n",
    "\n",
    "        subset = df[(df['metric'] == metric) & (df['dataset'] == dataset)]\n",
    "        \n",
    "        sns.boxplot(x='model', y='value', data=subset, palette=list(cd.values()), hue='model', legend=False, ax=ax, )\n",
    "        \n",
    "        for k, patch in enumerate(ax.patches):\n",
    "            if k in [3,4]:\n",
    "                r, g, b, a = patch.get_facecolor()\n",
    "                patch.set_facecolor((r, g, b, .5))\n",
    "\n",
    "        ax.set_xticks([]) \n",
    "        ax.set_xlabel(None) \n",
    "        ax.set_ylabel(None) \n",
    "        if j == 0:\n",
    "            ax.set_title(metric_names[metric])  # Set metric name as the title for the column\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(dataset_names[dataset])  # Set dataset name as the ylabel for the row\n",
    "\n",
    "handles = [plt.Line2D([0], [0], color=list(cd.values())[k], lw=5) for k in range(len(models))]\n",
    "\n",
    "\n",
    "fig.legend(handles, list(model_names.values()), loc='upper center', title='Models', ncol=len(models), bbox_to_anchor=(0.5, 1.035))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figures/generated_sequences_scores.pdf', format='pdf', dpi=1200, transparent=True,  bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
